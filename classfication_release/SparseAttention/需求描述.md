我想实现一种图像稀疏注意力方法。首先对于输入 [b n c] (n为像素点的个数)，将图片划分成网格[ b ,  h , r , w2 , c] (h 为heads ， r为region，即patch的个数， w2为每个patch内部的像素点个数) 。随后给定一个idx,shape为[b , h , r , k] ， 表示对于query中的每个region ， 在key、value中都有k个region相关联做注意力计算。
例如，先忽略batch 和 heads ，对于query中的一个patch其region下标为r，数据为[w2 ,c] ， 通过idx查询得到对应key中的的k个region，得到 [k , w2 , c] , 两者做attention计算， 得到attention map为[w2 , k , w2]。

随后利用得到的attention map（shape 为 b h r w2 k w2） ，对 （k w2） 维度进行softmax（每个像素做注意力交互的像素有kw2个）。然后使用同样的方法，使用idx将attention map与value对应部分相乘，得到输出out [b , h , r , w2 , c]。


由于使用pytorch实现可能会涉及到gather等操作浪费过多显存， 所以我想通过cuda编程实现这个功能，请你帮我实现该功能，尽可能节省计算资源。
我已经实现了获取attention map的部分，请你帮我实现 attention weighting的部分，由于我需要在训练过程调用该函数，所以需要考虑反向传播部分。
由于对于同一个region内部的元素来说，需要计算的注意力对象为同一批[k ,w2 ,c]，所以在实现获取attention map部分中，我使用了共享内存来存储这批像素以优化内存访问，在attention weighting部分，能否采用类似的方法加速计算？


目标：
实现一个融合（fused）的 CUDA 算子，用于完成注意力加权部分，也就是直接利用注意力权重、value 和 region 索引（idx）生成最终输出，而无需生成中间冗余的 gather 张量。


- **输入：**
  - **注意力权重（attention）：**  
    一个张量，其形状 为[b , h , r , w2 ,  k , w2]，这里 r 为大区域数，w2 为每个区域中的元素数 ， k 为每个区域对应topk个相关区域，w2表示每个region内部有w2个像素。
  - **Value张量（value）：**  
    形状为 \[B, h, r, w2, c\]，表示每个大区域内的细粒度特征。
  - **Region索引（idx）：**  
    形状为 \[B, heads, r, k\]，每个区域对应选择了 k 个最相关的区域。

- **操作：**
  - 直接根据 idx 从 value 张量中提取出对应区域（即获得一个张量形状为 \[B, heads, r, k, w2, c\] 的子张量），  
  - 与 attention 权重（形状可能为 \[B, heads, r*w2, k*w2\]）进行加权运算，  
  - 得到加权求和后的最终输出，输出形状应该是 \[B, heads, r*w2, c\]（每个查询 patch 得到一个最终的特征向量）。

输入为：
atten :
value: [b , h , r , w2 , c]
idx : [ b , h , r , k]

输出： 
out : [ b , h , r , w2 , c]


**总结描述：**

“我需要实现一个融合的 CUDA 算子，该算子直接接收注意力权重、value张量和表示选取区域的索引（idx），并输出最终的加权结果。算子的前向过程应利用 idx 从 value 中提取相应的区域，然后结合注意力权重进行加权求和，输出形状为 \[B, h, r , w2, c\]。整个算子需要避免生成中间重复的张量以降低显存占用，并实现对应的反向传播支持训练。”


你可以这样描述你的新需求，以便更准确地表达你想要实现的 **稀疏注意力权重加权部分（attention weighting）** 计算逻辑：



我想实现一种图像稀疏注意力方法。首先对于输入 [b n c] (n为像素点的个数)，将图片划分成网格[ b ,  h , r , w2 , c] (h 为heads ， r为region，即patch的个数， w2为每个patch内部的像素点个数) 。随后给定一个idx,shape为[b , h , r , k] ， 表示对于query中的每个region ， 在key、value中都有k个region相关联做注意力计算。
例如，先忽略batch 和 heads ，对于query中的一个patch其region下标为r，数据为[w2 ,c] ， 通过idx查询得到对应key中的的k个region，得到 [k , w2 , c] , 两者做attention计算， 得到attention map为[w2 , k , w2]。

随后利用得到的attention map（shape 为 b h r w2 k w2） ，对 （k w2） 维度进行softmax（每个像素做注意力交互的像素有kw2个）。然后使用同样的方法，使用idx将attention map与value对应部分相乘，得到输出out [b , h , r , w2 , c]。
我已经实现了获取attention map的部分，请你帮我实现 attention weighting的部分，也就是稀疏注意力机制中的 **加权输出部分**。  由于我需要在训练过程调用该函数，所以需要考虑反向传播部分。
在我的项目上，region大约为49个(划分成7*7), 在不同尺度下w2 的大小为[64 , 16] , k的大小为[12 , 12] , c的大小为[64,128]。

给定以下输入张量：

- `atten`：注意力权重，形状为 `[B, H, R, W2, K, W2]`
- `value`：value 向量，形状为 `[B, H, R, W2, C]`
- `idx`：用于检索与当前 query patch 关联的 K 个 key/value 区域，形状为 `[B, H, R, K]`

我希望输出：

- `out`：加权结果，形状为 `[B, H, R, W2, C]`

计算逻辑如下：

对于每个位置 `(b, h, r, i)` 中的 query 像素：

1. 查找该 region 所关联的 `K` 个 value 区域索引（由 `idx[b, h, r, j]` 给出）；
2. 对应这些 key 区域中每个像素 `(l)`，计算加权：
   \[
   \texttt{out}[b,h,r,i,c] = \sum_{j=0}^{K-1} \sum_{l=0}^{W2-1} \texttt{atten}[b,h,r,i,j,l] \cdot \texttt{value}[b,h,\texttt{idx}[b,h,r,j],l,c]
   \]

要求：

- 用 CUDA 编写高效 kernel；
- 避免不必要的 gather 或多次全局内存访问；
- 可以考虑使用共享内存优化
如果我的需求中有相互冲突或描述模糊的地方，请你先向我提问清楚后再开始。


👉 请帮我用 CUDA 实现上述 attention weighting 部分（atten × value），输出 out，尽可能优化内存访问。谢谢你！！！！！！！！！！！！！！！！！！！！！
