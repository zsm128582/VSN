| distributed init (rank 1): env://
| distributed init (rank 0): env://
Namespace(early_conv=False, conv_pos=False, use_ortho=False, batch_size=64, epochs=300, model='RMT_S', input_size=224, drop=0.0, drop_path=0.05, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, finetune='', data_path='/data2/ImageNet1k/data/', data_set='IMNET', inat_category='name', output_dir='save', device='cuda', seed=0, resume='', start_epoch=0, eval=False, dist_eval=True, num_workers=16, pin_mem=True, world_size=2, dist_url='env://', rank=0, gpu=0, distributed=True, dist_backend='nccl')
Creating model: RMT_S
VisRetNet(
  (patch_embed): PatchEmbed(
    (proj): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): GELU(approximate='none')
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): GELU(approximate='none')
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): GELU(approximate='none')
      (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layers): ModuleList(
    (0): BasicLayer(
      (Relpos): RetNetRelPos2d()
      (blocks): ModuleList(
        (0): RetBlock(
          (retention_layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionChunk(
            (q_proj): Linear(in_features=64, out_features=64, bias=True)
            (k_proj): Linear(in_features=64, out_features=64, bias=True)
            (v_proj): Linear(in_features=64, out_features=64, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64)
            )
            (out_proj): Linear(in_features=64, out_features=64, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.000)
          (final_layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
          )
        )
        (1): RetBlock(
          (retention_layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionChunk(
            (q_proj): Linear(in_features=64, out_features=64, bias=True)
            (k_proj): Linear(in_features=64, out_features=64, bias=True)
            (v_proj): Linear(in_features=64, out_features=64, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64)
            )
            (out_proj): Linear(in_features=64, out_features=64, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.005)
          (final_layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
          )
        )
        (2): RetBlock(
          (retention_layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionChunk(
            (q_proj): Linear(in_features=64, out_features=64, bias=True)
            (k_proj): Linear(in_features=64, out_features=64, bias=True)
            (v_proj): Linear(in_features=64, out_features=64, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64)
            )
            (out_proj): Linear(in_features=64, out_features=64, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.011)
          (final_layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
          )
        )
      )
      (downsample): PatchMerging(
        (reduction): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicLayer(
      (Relpos): RetNetRelPos2d()
      (blocks): ModuleList(
        (0): RetBlock(
          (retention_layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionChunk(
            (q_proj): Linear(in_features=128, out_features=128, bias=True)
            (k_proj): Linear(in_features=128, out_features=128, bias=True)
            (v_proj): Linear(in_features=128, out_features=128, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=128)
            )
            (out_proj): Linear(in_features=128, out_features=128, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.016)
          (final_layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          )
        )
        (1): RetBlock(
          (retention_layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionChunk(
            (q_proj): Linear(in_features=128, out_features=128, bias=True)
            (k_proj): Linear(in_features=128, out_features=128, bias=True)
            (v_proj): Linear(in_features=128, out_features=128, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=128)
            )
            (out_proj): Linear(in_features=128, out_features=128, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.021)
          (final_layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          )
        )
        (2): RetBlock(
          (retention_layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionChunk(
            (q_proj): Linear(in_features=128, out_features=128, bias=True)
            (k_proj): Linear(in_features=128, out_features=128, bias=True)
            (v_proj): Linear(in_features=128, out_features=128, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=128)
            )
            (out_proj): Linear(in_features=128, out_features=128, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (final_layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          )
        )
        (3): RetBlock(
          (retention_layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionChunk(
            (q_proj): Linear(in_features=128, out_features=128, bias=True)
            (k_proj): Linear(in_features=128, out_features=128, bias=True)
            (v_proj): Linear(in_features=128, out_features=128, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=128)
            )
            (out_proj): Linear(in_features=128, out_features=128, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.032)
          (final_layer_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          )
        )
      )
      (downsample): PatchMerging(
        (reduction): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): BasicLayer(
      (Relpos): RetNetRelPos2d()
      (blocks): ModuleList(
        (0): RetBlock(
          (retention_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionChunk(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
            )
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.038)
          (final_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=256, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=256, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
        )
        (1): RetBlock(
          (retention_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionChunk(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
            )
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.043)
          (final_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=256, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=256, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
        )
        (2): RetBlock(
          (retention_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionChunk(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
            )
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.048)
          (final_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=256, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=256, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
        )
        (3): RetBlock(
          (retention_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionChunk(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
            )
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.054)
          (final_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=256, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=256, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
        )
        (4): RetBlock(
          (retention_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionChunk(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
            )
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.059)
          (final_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=256, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=256, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
        )
        (5): RetBlock(
          (retention_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionChunk(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
            )
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.064)
          (final_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=256, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=256, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
        )
        (6): RetBlock(
          (retention_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionChunk(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
            )
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.070)
          (final_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=256, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=256, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
        )
        (7): RetBlock(
          (retention_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionChunk(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
            )
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.075)
          (final_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=256, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=256, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
        )
        (8): RetBlock(
          (retention_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionChunk(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
            )
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (final_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=256, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=256, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
        )
        (9): RetBlock(
          (retention_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionChunk(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
            )
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.086)
          (final_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=256, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=256, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
        )
        (10): RetBlock(
          (retention_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionChunk(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
            )
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.091)
          (final_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=256, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=256, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
        )
        (11): RetBlock(
          (retention_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionChunk(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
            )
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.096)
          (final_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=256, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=256, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
        )
        (12): RetBlock(
          (retention_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionChunk(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
            )
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.102)
          (final_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=256, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=256, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
        )
        (13): RetBlock(
          (retention_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionChunk(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
            )
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.107)
          (final_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=256, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=256, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
        )
        (14): RetBlock(
          (retention_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionChunk(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
            )
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.113)
          (final_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=256, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=256, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
        )
        (15): RetBlock(
          (retention_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionChunk(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
            )
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.118)
          (final_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=256, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=256, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
        )
        (16): RetBlock(
          (retention_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionChunk(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
            )
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.123)
          (final_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=256, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=256, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
        )
        (17): RetBlock(
          (retention_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionChunk(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
            )
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.129)
          (final_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=256, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=256, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
        )
      )
      (downsample): PatchMerging(
        (reduction): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): BasicLayer(
      (Relpos): RetNetRelPos2d()
      (blocks): ModuleList(
        (0): RetBlock(
          (retention_layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionAll(
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)
            )
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.134)
          (final_layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=512, out_features=1536, bias=True)
            (fc2): Linear(in_features=1536, out_features=512, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
        )
        (1): RetBlock(
          (retention_layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionAll(
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)
            )
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.139)
          (final_layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=512, out_features=1536, bias=True)
            (fc2): Linear(in_features=1536, out_features=512, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
        )
        (2): RetBlock(
          (retention_layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionAll(
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)
            )
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.145)
          (final_layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=512, out_features=1536, bias=True)
            (fc2): Linear(in_features=1536, out_features=512, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
        )
        (3): RetBlock(
          (retention_layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (retention): VisionRetentionAll(
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (lepe): DWConv2d(
              (conv): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)
            )
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.150)
          (final_layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (ffn): FeedForwardNetwork(
            (activation_dropout_module): Dropout(p=0.0, inplace=False)
            (dropout_module): Dropout(p=0.0, inplace=False)
            (fc1): Linear(in_features=512, out_features=1536, bias=True)
            (fc2): Linear(in_features=1536, out_features=512, bias=True)
            (dwconv): DWConv2d(
              (conv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
            )
          )
          (pos): DWConv2d(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
        )
      )
    )
  )
  (proj): Linear(in_features=512, out_features=1024, bias=True)
  (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (swish): MemoryEfficientSwish()
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
| module                        | #parameters or shape   | #flops     |
|:------------------------------|:-----------------------|:-----------|
| model                         | 26.967M                | 4.8G       |
|  patch_embed.proj             |  65.952K               |  0.302G    |
|   patch_embed.proj.0          |   0.896K               |   10.838M  |
|    patch_embed.proj.0.weight  |    (32, 3, 3, 3)       |            |
|    patch_embed.proj.0.bias    |    (32,)               |            |
|   patch_embed.proj.1          |   64                   |   0.803M   |
|    patch_embed.proj.1.weight  |    (32,)               |            |
|    patch_embed.proj.1.bias    |    (32,)               |            |
|   patch_embed.proj.3          |   9.248K               |   0.116G   |
|    patch_embed.proj.3.weight  |    (32, 32, 3, 3)      |            |
|    patch_embed.proj.3.bias    |    (32,)               |            |
|   patch_embed.proj.4          |   64                   |   0.803M   |
|    patch_embed.proj.4.weight  |    (32,)               |            |
|    patch_embed.proj.4.bias    |    (32,)               |            |
|   patch_embed.proj.6          |   18.496K              |   57.803M  |
|    patch_embed.proj.6.weight  |    (64, 32, 3, 3)      |            |
|    patch_embed.proj.6.bias    |    (64,)               |            |
|   patch_embed.proj.7          |   0.128K               |   0.401M   |
|    patch_embed.proj.7.weight  |    (64,)               |            |
|    patch_embed.proj.7.bias    |    (64,)               |            |
|   patch_embed.proj.9          |   36.928K              |   0.116G   |
|    patch_embed.proj.9.weight  |    (64, 64, 3, 3)      |            |
|    patch_embed.proj.9.bias    |    (64,)               |            |
|   patch_embed.proj.10         |   0.128K               |   0.401M   |
|    patch_embed.proj.10.weight |    (64,)               |            |
|    patch_embed.proj.10.bias   |    (64,)               |            |
|  layers                       |  25.349M               |  4.47G     |
|   layers.0                    |   0.239M               |   0.703G   |
|    layers.0.blocks            |    0.165M              |    0.645G  |
|    layers.0.downsample        |    74.112K             |    58.003M |
|   layers.1                    |   1.128M               |   0.752G   |
|    layers.1.blocks            |    0.832M              |    0.694G  |
|    layers.1.downsample        |    0.296M              |    57.903M |
|   layers.2                    |   13.337M              |   2.485G   |
|    layers.2.blocks            |    12.156M             |    2.427G  |
|    layers.2.downsample        |    1.181M              |    57.853M |
|   layers.3.blocks             |   10.646M              |   0.531G   |
|    layers.3.blocks.0          |    2.661M              |    0.133G  |
|    layers.3.blocks.1          |    2.661M              |    0.133G  |
|    layers.3.blocks.2          |    2.661M              |    0.133G  |
|    layers.3.blocks.3          |    2.661M              |    0.133G  |
|  proj                         |  0.525M                |  25.69M    |
|   proj.weight                 |   (1024, 512)          |            |
|   proj.bias                   |   (1024,)              |            |
|  norm                         |  2.048K                |  0.1M      |
|   norm.weight                 |   (1024,)              |            |
|   norm.bias                   |   (1024,)              |            |
|  head                         |  1.025M                |  1.024M    |
|   head.weight                 |   (1000, 1024)         |            |
|   head.bias                   |   (1000,)              |            |
number of params: 26967240
Start training for 300 epochs
Epoch: [0]  [    0/10008]  eta: 15:29:34  lr: 0.000001  loss: 6.8953 (6.8953)  time: 5.5730  data: 1.6592  max mem: 9906
Epoch: [0]  [   10/10008]  eta: 2:07:16  lr: 0.000001  loss: 6.9251 (6.9238)  time: 0.7638  data: 0.1510  max mem: 10152
Epoch: [0]  [   20/10008]  eta: 1:28:07  lr: 0.000001  loss: 6.9135 (6.9185)  time: 0.2772  data: 0.0002  max mem: 10152
Epoch: [0]  [   30/10008]  eta: 1:13:07  lr: 0.000001  loss: 6.9079 (6.9177)  time: 0.2615  data: 0.0002  max mem: 10152
Epoch: [0]  [   40/10008]  eta: 1:05:31  lr: 0.000001  loss: 6.9153 (6.9173)  time: 0.2527  data: 0.0002  max mem: 10152
Epoch: [0]  [   50/10008]  eta: 1:00:12  lr: 0.000001  loss: 6.9130 (6.9176)  time: 0.2434  data: 0.0002  max mem: 10152
Epoch: [0]  [   60/10008]  eta: 0:56:36  lr: 0.000001  loss: 6.9214 (6.9195)  time: 0.2327  data: 0.0002  max mem: 10152
Epoch: [0]  [   70/10008]  eta: 0:54:36  lr: 0.000001  loss: 6.9286 (6.9200)  time: 0.2454  data: 0.0002  max mem: 10152
Epoch: [0]  [   80/10008]  eta: 0:53:04  lr: 0.000001  loss: 6.9242 (6.9198)  time: 0.2577  data: 0.0002  max mem: 10152
Epoch: [0]  [   90/10008]  eta: 0:51:57  lr: 0.000001  loss: 6.9200 (6.9193)  time: 0.2600  data: 0.0002  max mem: 10152
Epoch: [0]  [  100/10008]  eta: 0:50:55  lr: 0.000001  loss: 6.9124 (6.9192)  time: 0.2583  data: 0.0002  max mem: 10152
Epoch: [0]  [  110/10008]  eta: 0:49:48  lr: 0.000001  loss: 6.9117 (6.9187)  time: 0.2456  data: 0.0002  max mem: 10152
Epoch: [0]  [  120/10008]  eta: 0:49:15  lr: 0.000001  loss: 6.9151 (6.9187)  time: 0.2512  data: 0.0002  max mem: 10152
Epoch: [0]  [  130/10008]  eta: 0:48:42  lr: 0.000001  loss: 6.9151 (6.9187)  time: 0.2617  data: 0.0002  max mem: 10152
Epoch: [0]  [  140/10008]  eta: 0:48:17  lr: 0.000001  loss: 6.9220 (6.9191)  time: 0.2613  data: 0.0002  max mem: 10152
Epoch: [0]  [  150/10008]  eta: 0:47:47  lr: 0.000001  loss: 6.9260 (6.9200)  time: 0.2584  data: 0.0002  max mem: 10152
Epoch: [0]  [  160/10008]  eta: 0:47:21  lr: 0.000001  loss: 6.9325 (6.9203)  time: 0.2530  data: 0.0001  max mem: 10152
Epoch: [0]  [  170/10008]  eta: 0:46:59  lr: 0.000001  loss: 6.9157 (6.9207)  time: 0.2541  data: 0.0002  max mem: 10152
Epoch: [0]  [  180/10008]  eta: 0:46:38  lr: 0.000001  loss: 6.9157 (6.9207)  time: 0.2539  data: 0.0002  max mem: 10152
Epoch: [0]  [  190/10008]  eta: 0:46:25  lr: 0.000001  loss: 6.9146 (6.9204)  time: 0.2590  data: 0.0002  max mem: 10152
